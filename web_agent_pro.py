import streamlit as st
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict
import os
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from tavily import TavilyClient
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader


st.set_page_config(page_title="å®Œå…¨ä½“è€ç‹ (åŒè„‘é©±åŠ¨)", page_icon="ğŸ§ ")
st.title("ğŸ§  å®Œå…¨ä½“è€ç‹ (å…¬ç½‘ + ç§æœ‰çŸ¥è¯†åº“)")

# åˆå§‹åŒ–æ ¸å¿ƒå¼•æ“ (å¤§æ¨¡å‹ + æœç´¢å¼•æ“)
llm = ChatOpenAI(
    api_key=st.secrets["API_KEY"], 
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    model="qwen-max"
)
tavily_client = TavilyClient(api_key=st.secrets["TAVILY_API_KEY"])

# ğŸ§  åˆå§‹åŒ–è€ç‹çš„æ°¸ä¹…è®°å¿†æ”¯æ¶
if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None
if 'learned_files' not in st.session_state:
    st.session_state.learned_files = []
# ğŸ‘‡ ğŸš¨ æ–°å¢ï¼šå‡†å¤‡ä¸€ä¸ªå¤§ä¹¦åŒ…ï¼Œç”¨æ¥è£…æ•´æœ¬ä¹¦çš„çº¯æ–‡æœ¬
if 'raw_text' not in st.session_state:
    st.session_state.raw_text = ""

# åŠ¨æ€è¯»å–å¹¶è½¬æ¢ PDF çš„æ ¸å¿ƒå‡½æ•°
@st.cache_resource(show_spinner=False)
def process_new_document(file_bytes, file_name):
    # æå–æ–‡ä»¶çš„åç¼€å (æ¯”å¦‚ .pdf, .txt)
    ext = os.path.splitext(file_name)[1].lower()
    
    # åŠ¨æ€ç”Ÿæˆä¸´æ—¶æ–‡ä»¶åï¼ˆä¿ç•™åŸåç¼€ï¼‰
    temp_file_path = f"temp_upload{ext}"
    with open(temp_file_path, "wb") as f:
        f.write(file_bytes)
        
    # ğŸš¨ æ ¸å¿ƒè·¯ç”±é€»è¾‘ï¼šæ ¹æ®ä¸åŒæ ¼å¼è°ƒç”¨ä¸åŒçš„è§£æå™¨
    if ext == ".pdf":
        loader = PyPDFLoader(temp_file_path)
    elif ext in [".txt", ".md", ".csv"]:
        # çº¯æ–‡æœ¬ç±»çš„æ–‡ä»¶ï¼Œç”¨ TextLoader é€šæ€
        loader = TextLoader(temp_file_path, encoding='utf-8')
    else:
        raise ValueError(f"å“å‘€ï¼Œè€ç‹è¿˜ä¸è®¤è¯† {ext} è¿™ç§æ ¼å¼çš„æ–‡ä»¶ï¼")
        
    docs = loader.load()
    
    # æå–å®Œæ•´çº¯æ–‡æœ¬
    full_text = "\n".join([doc.page_content for doc in docs])
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40)
    splits = text_splitter.split_documents(docs)
    
    embeddings = DashScopeEmbeddings(
        dashscope_api_key=st.secrets["API_KEY"], 
        model="text-embedding-v3" 
    )
    vectorstore = FAISS.from_documents(splits, embeddings)
    
    return vectorstore, full_text

# ==========================================
# ğŸš¨ ç»ˆæè¿›åŒ–ï¼šä¾§è¾¹æ ä¸è®°å¿†èåˆæ“ä½œå°
# ==========================================
with st.sidebar:
    # åŠ åœ¨ä¾§è¾¹æ çš„æœ€ä¸‹é¢
    st.write("---")
    st.header("âš™ï¸ å›¢é˜Ÿåå¥½è®¾ç½®")
    need_translate = st.checkbox("ğŸŒ å¬å”¤æ¸¡è¾¹ (å°†è€ç‹çš„å›ç­”ç¿»è¯‘ä¸ºçº¯æ­£æ—¥æ–‡)", value=False)
    st.header("ğŸ“‚ è€ç‹çš„æ°¸ä¹…è®°å¿†åº“")
    
    # ğŸš¨ æ ¸å¿ƒä¿®æ”¹ 1ï¼šæ”¾å®½æ ¼å¼é™åˆ¶ï¼Œå¹¶å¼€å¯ accept_multiple_files=True
    uploaded_files = st.file_uploader(
        "æ‰¹é‡ä¸Šä¼ ç§˜ç± (æ”¯æŒ PDF/TXT/MD)", 
        type=["pdf", "txt", "md"], 
        accept_multiple_files=True  # é­”æ³•å¼€å…³åœ¨è¿™é‡Œï¼
    )
    
    if st.button("ğŸ§  å¼€å§‹æ‰¹é‡èåˆå­¦ä¹ ") and uploaded_files:
        # ğŸš¨ æ ¸å¿ƒä¿®æ”¹ 2ï¼šæŠŠä¼ å…¥çš„åˆ—è¡¨åšä¸ª for å¾ªç¯ï¼ŒæŒ¨ä¸ªåƒæ‰
        for uploaded_file in uploaded_files:
            if uploaded_file.name in st.session_state.learned_files:
                st.warning(f"ã€Š{uploaded_file.name}ã€‹è€ç‹å·²ç»å€’èƒŒå¦‚æµå•¦ï¼Œè·³è¿‡ï¼")
                continue # å­¦è¿‡çš„ç›´æ¥è·³è¿‡ï¼Œå­¦ä¸‹ä¸€æœ¬
                
            with st.spinner(f"æ­£åœ¨å°†ã€Š{uploaded_file.name}ã€‹èå…¥å¤§è„‘..."):
                try:
                    # è°ƒç”¨åˆšæ‰å†™å¥½çš„å…¨æ ¼å¼è§£æå™¨
                    new_db, new_text = process_new_document(uploaded_file.getvalue(), uploaded_file.name)
                    
                    if st.session_state.vectorstore is None:
                        st.session_state.vectorstore = new_db
                    else:
                        st.session_state.vectorstore.merge_from(new_db)
                    
                    st.session_state.raw_text += f"\n\n---ã€Š{uploaded_file.name}ã€‹---\n\n{new_text}"
                    st.session_state.learned_files.append(uploaded_file.name)
                    st.success(f"âœ… ã€Š{uploaded_file.name}ã€‹èåˆå®Œæ¯•ï¼")
                except Exception as e:
                    st.error(f"âŒ èåˆã€Š{uploaded_file.name}ã€‹æ—¶å‡ºé”™ï¼š{str(e)}")

    if st.session_state.learned_files:
        st.write("---")
        st.write("ğŸ“š ç›®å‰å·²æŒæ¡çš„çŸ¥è¯†ï¼š")
        for f_name in st.session_state.learned_files:
            st.caption(f"â€¢ {f_name}")

# ä¸‹é¢ä¿ç•™ä½ çš„ @tool æŠ€èƒ½ä»£ç å’ŒèŠå¤©ç•Œé¢ä»£ç ï¼Œä¸éœ€è¦åŠ¨ï¼

# ==========================================
# ğŸ› ï¸ æŠ€èƒ½ 1ï¼šå…¬ç½‘æœç´¢ (ä¿æŒä¸å˜)
# ==========================================
@tool
def web_search(query: str) -> str:
    """å½“éœ€è¦æŸ¥è¯¢å®æ—¶ä¿¡æ¯ã€æ–°é—»ã€ä¸çŸ¥é“çš„å®¢è§‚çŸ¥è¯†æ—¶ï¼Œè°ƒç”¨æ­¤å·¥å…·å…¨ç½‘æœç´¢ã€‚"""
    try:
        response = tavily_client.search(query=query, search_depth="basic", max_results=2)
        return "\n\n".join([f"æ ‡é¢˜: {res['title']}\nå†…å®¹: {res['content']}" for res in response['results']])
    except Exception as e:
        return f"æœç´¢å¤±è´¥ï¼š{str(e)}"

# ==========================================
# ğŸ› ï¸ æŠ€èƒ½ 2ï¼šç§æœ‰çŸ¥è¯†åº“æœç´¢ (ç ´é™¤çº¿ç¨‹å£å’ç‰ˆ)
# ==========================================
# ğŸš¨ æ ¸å¿ƒé­”æ³•ï¼šåœ¨ä¸»çº¿ç¨‹é‡Œå…ˆæŠŠè„‘å­æ‹¿å‡ºæ¥ï¼Œæ”¾è¿›ä¸€ä¸ªæ™®é€šå˜é‡é‡Œï¼Œè®©å­çº¿ç¨‹ä¹Ÿèƒ½æ‘¸å¾—åˆ°
GLOBAL_BRAIN = st.session_state.get('vectorstore', None)

@tool
def search_internal_doc(query: str) -> str:
    """å½“ç”¨æˆ·è¯¢é—®å…³äºä¸Šä¼ çš„PDFæ–‡ä»¶ã€å†…éƒ¨çŸ¥è¯†ã€å¤ä¹ èµ„æ–™æ—¶ï¼Œè°ƒç”¨æ­¤å·¥å…·ã€‚"""
    if GLOBAL_BRAIN is None:
        return "è¯·ç¤¼è²Œåœ°å‘Šè¯‰ç”¨æˆ·ï¼šè€ç‹ç›®å‰è„‘å­é‡Œç©ºç©ºå¦‚ä¹Ÿï¼Œè¯·å…ˆä¸Šä¼  PDF èµ„æ–™ï¼"
    
    retriever = GLOBAL_BRAIN.as_retriever()
    results = retriever.invoke(query)
    return "\n\n".join([res.page_content for res in results])
# ==========================================
# ğŸ› ï¸ æŠ€èƒ½ 3ï¼šå…¨å±€æ–‡æ¡£åˆ†æ (ç ´é™¤çº¿ç¨‹å£å’ç‰ˆ)
# ==========================================
# ğŸš¨ æ ¸å¿ƒé­”æ³•ï¼šæå‰æŠŠçº¯æ–‡æœ¬æ‹¿å‡ºæ¥ï¼Œä¾›å­çº¿ç¨‹éšæ—¶å–ç”¨
GLOBAL_RAW_TEXT = st.session_state.get('raw_text', "")

@tool
def analyze_whole_document(query: str) -> str:
    """å½“ç”¨æˆ·è¦æ±‚â€œæ€»ç»“å…¨æ–‡â€ã€â€œæ•´ç†æ€ç»´å¯¼å›¾â€ã€â€œæå–å¤§çº²â€ç­‰æ¶‰åŠå®è§‚å…¨å±€åˆ†ææ—¶ï¼Œå¼ºåˆ¶è°ƒç”¨æ­¤å·¥å…·ã€‚"""
    if not GLOBAL_RAW_TEXT:
        return "è€ç‹è„‘å­é‡Œè¿˜æ²¡æœ‰å®Œæ•´çš„æ–‡æ¡£ï¼Œè¯·å…ˆä¸Šä¼  PDF èµ„æ–™ï¼"
    
    # æˆªå–å‰ 30000 ä¸ªå­—ç¬¦
    text_to_analyze = GLOBAL_RAW_TEXT[:30000]
    
    summary_llm = ChatOpenAI(
        api_key=st.secrets["API_KEY"], 
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        model="qwen-max"
    )
    
    prompt = f"ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ç³»ç»Ÿæ¶æ„å¸ˆã€‚è¯·åŸºäºä»¥ä¸‹æˆ‘æä¾›çš„å®Œæ•´æ–‡æ¡£å†…å®¹ï¼Œå®Œæˆç”¨æˆ·çš„ä»»åŠ¡ï¼š\n\nç”¨æˆ·ä»»åŠ¡ï¼š{query}\n\næ–‡æ¡£æ ¸å¿ƒå†…å®¹ï¼š\n{text_to_analyze}"
    
    response = summary_llm.invoke(prompt)
    return response.content

# ==========================================
# ğŸ¢ AI åˆ›ä¸šå…¬å¸ï¼šå¤š Agent åä½œç³»ç»Ÿæ¶æ„
# ==========================================


# 1. å®šä¹‰å…¬å¸çš„â€œå…±äº«é»‘æ¿â€ (State)
class AgentState(TypedDict):
    messages: Annotated[list, add_messages]
    need_translate: bool  # ğŸš¨ æ–°å¢ï¼šè®°å½•è€æ¿æ˜¯å¦éœ€è¦ç¿»è¯‘çš„æ—¨æ„

# 2. å®ä¾‹åŒ–ä¸€å·å‘˜å·¥ï¼šã€ç ”ç©¶å‘˜è€ç‹ã€‘ (ä»–å¸¦ç€é‚£ä¸‰ä¸ªæŠ€èƒ½å·¥å…·å¹²æ´»)
researcher_agent = create_react_agent(llm, [web_search, search_internal_doc, analyze_whole_document])

def researcher_node(state: AgentState):
    """è€ç‹çš„å·¥ä½œæµï¼šæ¥å• -> ç”¨å·¥å…·æŸ¥èµ„æ–™ -> æ•´ç†å‡ºä¸­æ–‡æŠ€æœ¯å¤§çº²"""
    result = researcher_agent.invoke({"messages": state["messages"]})
    return {"messages": [result["messages"][-1]]}

# 3. å®ä¾‹åŒ–äºŒå·å‘˜å·¥ï¼šã€æ—¥ç±ç¿»è¯‘å®˜æ¸¡è¾¹ã€‘
def translator_node(state: AgentState):
    """æ¸¡è¾¹çš„å·¥ä½œæµï¼šæ‹¿åˆ°è€ç‹çš„ä¸­æ–‡å¤§çº² -> è½¬åŒ–ä¸ºçº¯æ­£çš„æ—¥æœ¬ IT èŒåœºæŠ¥å‘Š"""
    laowang_report = state["messages"][-1].content
    
    sys_prompt = """ä½ å«æ¸¡è¾¹ï¼Œæ˜¯ä¸€ä½åœ¨ä¸œäº¬æ¶©è°·å·¥ä½œäº†10å¹´çš„èµ„æ·±ITç³»ç»Ÿæ¶æ„å¸ˆã€‚
    è¯·æ¥æ”¶ä¸‹é¢è¿™ä»½æ¥è‡ªä¸­æ–‡ç ”ç©¶å‘˜çš„æŠ€æœ¯æŠ¥å‘Šï¼Œå°†å…¶å®Œç¾ç¿»è¯‘å¹¶æ¶¦è‰²ä¸ºã€åœ°é“ã€ä¸“ä¸šçš„æ—¥æ–‡ IT ä¸šåŠ¡æŠ¥å‘Šã€‘ã€‚
    è¦æ±‚ï¼š
    1. å¿…é¡»ä½¿ç”¨æ ‡å‡† N2/N1 çº§åˆ«çš„æ—¥æ–‡å•†åŠ¡/IT æœ¯è¯­ã€‚
    2. ä¿æŒåŸæœ‰çš„æ€ç»´å¯¼å›¾æˆ–å±‚çº§å¤§çº²æ ¼å¼ï¼Œæ’ç‰ˆè¦æå…¶æ¸…æ™°ã€‚
    3. åœ¨å¼€å¤´ç”¨æ—¥æ–‡è·Ÿç”¨æˆ·æ‰“ä¸ªæ‹›å‘¼ï¼ˆæ¯”å¦‚ï¼šãŠç–²ã‚Œæ§˜ã§ã™ã€æ¸¡è¾ºã§ã™...ï¼‰ã€‚"""
    
    response = llm.invoke([
        {"role": "system", "content": sys_prompt}, 
        {"role": "user", "content": f"è¯·ç¿»è¯‘è¿™ä»½æŠ¥å‘Šï¼š\n{laowang_report}"}
    ])
    return {"messages": [response]}
# ğŸ‘‡ ğŸš¨ æ–°å¢ï¼šè°ƒåº¦å‘˜å‡½æ•°
def route_after_research(state: AgentState):
    """æ ¹æ®è€æ¿çš„æ—¨æ„ï¼Œå†³å®šè€ç‹å¹²å®Œæ´»åæ˜¯ç›´æ¥äº¤å·®ï¼Œè¿˜æ˜¯é€’äº¤ç»™æ¸¡è¾¹"""
    if state.get("need_translate", False):
        return "Translator"
    else:
        return END

# 4. åŒ…å·¥å¤´æ’ç­ï¼šç”¨ Graph æŠŠå‘˜å·¥è¿æˆæµæ°´çº¿
workflow = StateGraph(AgentState)

workflow.add_node("Researcher", researcher_node)
workflow.add_node("Translator", translator_node)

workflow.add_edge(START, "Researcher")

# ğŸ‘‡ ğŸš¨ æ ¸å¿ƒä¿®æ”¹ï¼šæŠŠåŸæ¥çš„ workflow.add_edge("Researcher", "Translator") åˆ æ‰ï¼Œæ¢æˆè¿™è¡Œâ€œæ¡ä»¶è¿çº¿â€ï¼
workflow.add_conditional_edges("Researcher", route_after_research, {"Translator": "Translator", END: END})

workflow.add_edge("Translator", END)

# æ­£å¼æŒ‚ç‰Œè¥ä¸šï¼
multi_agent_app = workflow.compile()

# ---------------- ä¸‹é¢æ˜¯ç½‘é¡µç•Œé¢çš„å¸¸è§„é€»è¾‘ ----------------
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¹½é»˜çš„å…¨èƒ½AIåŠ©ç†è€ç‹ã€‚"}]

for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

if user_input := st.chat_input("è¯•è¯•è¿æ‹›ï¼šä»Šå¤©çš„å¾®åšçƒ­æœæ˜¯ä»€ä¹ˆï¼Ÿé‚£è½¯ä»¶è®¾è®¡å¸ˆçš„å£è¯€å‘¢ï¼Ÿ"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        # æ™ºèƒ½æç¤ºè¯­ï¼šæ ¹æ®å¼€å…³çŠ¶æ€æ˜¾ç¤ºè°åœ¨å¹²æ´»
        status_text = "è€ç‹æ­£åœ¨æŸ¥é˜…èµ„æ–™ï¼Œæ¸¡è¾¹æ­£åœ¨å‡†å¤‡æ—¥æ–‡ç¿»è¯‘..." if need_translate else "è€ç‹æ­£åœ¨ç–¯ç‹‚é€Ÿè¯»å¹¶æ€»ç»“..."
        
        with st.spinner(status_text):
            # ğŸš¨ æ ¸å¿ƒä¿®æ”¹ï¼šæŠŠ need_translate ä¼ è¿›å…¬å¸é»‘æ¿ï¼
            response = multi_agent_app.invoke({
                "messages": st.session_state.messages,
                "need_translate": need_translate 
            })
            ai_reply = response["messages"][-1].content
            st.markdown(ai_reply)
            
    st.session_state.messages.append({"role": "assistant", "content": ai_reply})