import streamlit as st
import os
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from tavily import TavilyClient
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import FAISS

st.set_page_config(page_title="å®Œå…¨ä½“è€ç‹ (åŒè„‘é©±åŠ¨)", page_icon="ğŸ§ ")
st.title("ğŸ§  å®Œå…¨ä½“è€ç‹ (å…¬ç½‘ + ç§æœ‰çŸ¥è¯†åº“)")

# åˆå§‹åŒ–æ ¸å¿ƒå¼•æ“ (å¤§æ¨¡å‹ + æœç´¢å¼•æ“)
llm = ChatOpenAI(
    api_key=st.secrets["API_KEY"], 
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    model="qwen-max"
)
tavily_client = TavilyClient(api_key=st.secrets["TAVILY_API_KEY"])

# ğŸ§  åˆå§‹åŒ–è€ç‹çš„æ°¸ä¹…è®°å¿†æ”¯æ¶
if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None
if 'learned_files' not in st.session_state:
    st.session_state.learned_files = []

# åŠ¨æ€è¯»å–å¹¶è½¬æ¢ PDF çš„æ ¸å¿ƒå‡½æ•°
@st.cache_resource(show_spinner=False)
def process_new_pdf(file_bytes, file_name):
    with open("temp_upload.pdf", "wb") as f:
        f.write(file_bytes)
        
    loader = PyPDFLoader("temp_upload.pdf") 
    docs = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40)
    splits = text_splitter.split_documents(docs)
    
    embeddings = DashScopeEmbeddings(
        dashscope_api_key=st.secrets["API_KEY"], 
        model="text-embedding-v3" 
    )
    return FAISS.from_documents(splits, embeddings)

# ==========================================
# ğŸš¨ ç»ˆæè¿›åŒ–ï¼šä¾§è¾¹æ ä¸è®°å¿†èåˆæ“ä½œå°
# ==========================================
with st.sidebar:
    st.header("ğŸ“‚ è€ç‹çš„æ°¸ä¹…è®°å¿†åº“")
    uploaded_file = st.file_uploader("ä¸Šä¼ ã€Šè½¯ä»¶è®¾è®¡å¸ˆã€‹æ–°èµ„æ–™", type=["pdf"])
    
    if st.button("ğŸ§  å¼€å§‹èåˆå­¦ä¹ ") and uploaded_file is not None:
        if uploaded_file.name in st.session_state.learned_files:
            st.warning(f"è¿™ä»½ã€Š{uploaded_file.name}ã€‹è€ç‹å·²ç»å€’èƒŒå¦‚æµå•¦ï¼")
        else:
            with st.spinner(f"æ­£åœ¨å°†ã€Š{uploaded_file.name}ã€‹èå…¥å¤§è„‘..."):
                try:
                    new_db = process_new_pdf(uploaded_file.getvalue(), uploaded_file.name)
                    
                    if st.session_state.vectorstore is None:
                        st.session_state.vectorstore = new_db
                    else:
                        st.session_state.vectorstore.merge_from(new_db)
                    
                    st.session_state.learned_files.append(uploaded_file.name)
                    st.success(f"âœ… æˆåŠŸèåˆï¼ç›®å‰å·²æŒæ¡ {len(st.session_state.learned_files)} ä»½èµ„æ–™ã€‚")
                except Exception as e:
                    st.error(f"âŒ æŠ“åˆ°çœŸå‡¶äº†ï¼çœŸå®æŠ¥é”™æ˜¯ï¼š{str(e)}")

    if st.session_state.learned_files:
        st.write("---")
        st.write("ğŸ“š ç›®å‰å·²æŒæ¡çš„çŸ¥è¯†ï¼š")
        for f_name in st.session_state.learned_files:
            st.caption(f"â€¢ {f_name}")

# ä¸‹é¢ä¿ç•™ä½ çš„ @tool æŠ€èƒ½ä»£ç å’ŒèŠå¤©ç•Œé¢ä»£ç ï¼Œä¸éœ€è¦åŠ¨ï¼

# ==========================================
# ğŸ› ï¸ æŠ€èƒ½ 1ï¼šå…¬ç½‘æœç´¢ (ä¿æŒä¸å˜)
# ==========================================
@tool
def web_search(query: str) -> str:
    """å½“éœ€è¦æŸ¥è¯¢å®æ—¶ä¿¡æ¯ã€æ–°é—»ã€ä¸çŸ¥é“çš„å®¢è§‚çŸ¥è¯†æ—¶ï¼Œè°ƒç”¨æ­¤å·¥å…·å…¨ç½‘æœç´¢ã€‚"""
    try:
        response = tavily_client.search(query=query, search_depth="basic", max_results=2)
        return "\n\n".join([f"æ ‡é¢˜: {res['title']}\nå†…å®¹: {res['content']}" for res in response['results']])
    except Exception as e:
        return f"æœç´¢å¤±è´¥ï¼š{str(e)}"

# ==========================================
# ğŸ› ï¸ æŠ€èƒ½ 2ï¼šç§æœ‰çŸ¥è¯†åº“æœç´¢ (ç ´é™¤çº¿ç¨‹å£å’ç‰ˆ)
# ==========================================
# ğŸš¨ æ ¸å¿ƒé­”æ³•ï¼šåœ¨ä¸»çº¿ç¨‹é‡Œå…ˆæŠŠè„‘å­æ‹¿å‡ºæ¥ï¼Œæ”¾è¿›ä¸€ä¸ªæ™®é€šå˜é‡é‡Œï¼Œè®©å­çº¿ç¨‹ä¹Ÿèƒ½æ‘¸å¾—åˆ°
GLOBAL_BRAIN = st.session_state.get('vectorstore', None)

@tool
def search_internal_doc(query: str) -> str:
    """å½“ç”¨æˆ·è¯¢é—®å…³äºä¸Šä¼ çš„PDFæ–‡ä»¶ã€å†…éƒ¨çŸ¥è¯†ã€å¤ä¹ èµ„æ–™æ—¶ï¼Œè°ƒç”¨æ­¤å·¥å…·ã€‚"""
    if GLOBAL_BRAIN is None:
        return "è¯·ç¤¼è²Œåœ°å‘Šè¯‰ç”¨æˆ·ï¼šè€ç‹ç›®å‰è„‘å­é‡Œç©ºç©ºå¦‚ä¹Ÿï¼Œè¯·å…ˆä¸Šä¼  PDF èµ„æ–™ï¼"
    
    retriever = GLOBAL_BRAIN.as_retriever()
    results = retriever.invoke(query)
    return "\n\n".join([res.page_content for res in results])

# å°†ä¸¤ä¸ªæŠ€èƒ½è£…è¿›å¤§è„‘
agent_executor = create_react_agent(llm, [web_search, search_internal_doc])


# ---------------- ä¸‹é¢æ˜¯ç½‘é¡µç•Œé¢çš„å¸¸è§„é€»è¾‘ ----------------
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¹½é»˜çš„å…¨èƒ½AIåŠ©ç†è€ç‹ã€‚"}]

for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

if user_input := st.chat_input("è¯•è¯•è¿æ‹›ï¼šä»Šå¤©çš„å¾®åšçƒ­æœæ˜¯ä»€ä¹ˆï¼Ÿé‚£è½¯ä»¶è®¾è®¡å¸ˆçš„å£è¯€å‘¢ï¼Ÿ"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("è€ç‹æ­£åœ¨å·¦å³è„‘åŒæ—¶è¿è½¬..."):
            response = agent_executor.invoke({"messages": st.session_state.messages})
            ai_reply = response["messages"][-1].content
            st.markdown(ai_reply)
            
    st.session_state.messages.append({"role": "assistant", "content": ai_reply})