import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from tavily import TavilyClient
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import FAISS

st.set_page_config(page_title="å®Œå…¨ä½“è€ç‹ (åŒè„‘é©±åŠ¨)", page_icon="ğŸ§ ")
st.title("ğŸ§  å®Œå…¨ä½“è€ç‹ (å…¬ç½‘ + ç§æœ‰çŸ¥è¯†åº“)")

# 1. åˆå§‹åŒ–
llm = ChatOpenAI(
    api_key=st.secrets["API_KEY"], 
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    model="qwen-coder-plus"
)
tavily_client = TavilyClient(api_key=st.secrets["TAVILY_API_KEY"])

# ==========================================
# ğŸš¨ æ–°å¢é­”æ³•ï¼šæŠŠçŸ¥è¯†åº“ç¼“å­˜åœ¨ç½‘é¡µå†…å­˜é‡Œï¼
# ä½¿ç”¨ @st.cache_resource é˜²æ­¢æ¯æ¬¡èŠå¤©éƒ½é‡æ–°è¯»å–æ–‡ä»¶
# ==========================================
@st.cache_resource
def load_knowledge_base():
    loader = TextLoader("knowledge.txt", encoding="utf-8")
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)
    splits = text_splitter.split_documents(docs)
    embeddings = DashScopeEmbeddings(
        dashscope_api_key=st.secrets["API_KEY"], 
        model="text-embedding-v3" 
    )
    return FAISS.from_documents(splits, embeddings)

vectorstore = load_knowledge_base()

# ==========================================
# ğŸ› ï¸ æŠ€èƒ½ 1ï¼šå…¬ç½‘æœç´¢
# ==========================================
@tool
def web_search(query: str) -> str:
    """å½“éœ€è¦æŸ¥è¯¢å®æ—¶ä¿¡æ¯ã€æ–°é—»ã€ä¸çŸ¥é“çš„å®¢è§‚çŸ¥è¯†æ—¶ï¼Œè°ƒç”¨æ­¤å·¥å…·å…¨ç½‘æœç´¢ã€‚"""
    try:
        response = tavily_client.search(query=query, search_depth="basic", max_results=2)
        return "\n\n".join([f"æ ‡é¢˜: {res['title']}\nå†…å®¹: {res['content']}" for res in response['results']])
    except Exception as e:
        return f"æœç´¢å¤±è´¥ï¼š{str(e)}"

# ==========================================
# ğŸ› ï¸ æŠ€èƒ½ 2ï¼šç§æœ‰çŸ¥è¯†åº“æœç´¢ (RAG)
# ==========================================
@tool
def search_internal_doc(query: str) -> str:
    """å½“ç”¨æˆ·è¯¢é—®å…³äº'è½¯ä»¶è®¾è®¡å¸ˆ'è€ƒè¯•å£è¯€ã€æå››è€æ¿çš„æ—¥è¯­å­¦ä¹ æƒ…å†µã€æˆ–è€…ç»å¯†æ¡£æ¡ˆæ—¶ï¼Œå¿…é¡»è°ƒç”¨æ­¤å·¥å…·æŸ¥è¯¢å†…éƒ¨çŸ¥è¯†åº“ã€‚"""
    retriever = vectorstore.as_retriever()
    results = retriever.invoke(query)
    return "\n\n".join([res.page_content for res in results])

# å°†ä¸¤ä¸ªæŠ€èƒ½éƒ½è£…è¿›å¤§è„‘
agent_executor = create_react_agent(llm, [web_search, search_internal_doc])

# ---------------- ä¸‹é¢æ˜¯ç½‘é¡µç•Œé¢çš„å¸¸è§„é€»è¾‘ ----------------
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¹½é»˜çš„å…¨èƒ½AIåŠ©ç†è€ç‹ã€‚"}]

for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

if user_input := st.chat_input("è¯•è¯•è¿æ‹›ï¼šä»Šå¤©çš„å¾®åšçƒ­æœæ˜¯ä»€ä¹ˆï¼Ÿé‚£è½¯ä»¶è®¾è®¡å¸ˆçš„å£è¯€å‘¢ï¼Ÿ"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("è€ç‹æ­£åœ¨å·¦å³è„‘åŒæ—¶è¿è½¬..."):
            response = agent_executor.invoke({"messages": st.session_state.messages})
            ai_reply = response["messages"][-1].content
            st.markdown(ai_reply)
            
    st.session_state.messages.append({"role": "assistant", "content": ai_reply})