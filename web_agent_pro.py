import streamlit as st
import os  # ğŸ‘ˆ æ–°å¢ï¼šç”¨äºä¿å­˜ä¸´æ—¶ä¸Šä¼ çš„æ–‡ä»¶
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from tavily import TavilyClient
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import FAISS

st.set_page_config(page_title="å®Œå…¨ä½“è€ç‹ (åŒè„‘é©±åŠ¨)", page_icon="ğŸ§ ")
st.title("ğŸ§  å®Œå…¨ä½“è€ç‹ (å…¬ç½‘ + ç§æœ‰çŸ¥è¯†åº“)")

# åˆå§‹åŒ–æ ¸å¿ƒå¼•æ“ (å¤§æ¨¡å‹ + æœç´¢å¼•æ“)
llm = ChatOpenAI(
    api_key=st.secrets["API_KEY"], 
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    model="qwen-max"
) # ğŸ‘ˆ æ£€æŸ¥è¿™é‡Œï¼æ˜¯ä¸æ˜¯å°‘äº†è¿™ä¸ªåæ‹¬å·ï¼Ÿ
# å¦‚æœå†…å­˜é‡Œè¿˜æ²¡æœ‰è„‘å­ï¼Œå…ˆåˆå§‹åŒ–ä¸€ä¸ªç©ºçš„
# åˆå§‹åŒ–è€ç‹çš„è®°å¿†æ”¯æ¶ï¼ˆæ”¾åœ¨è®¾ç½® API_KEY çš„é™„è¿‘å³å¯ï¼‰
if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None
if 'learned_files' not in st.session_state:
    st.session_state.learned_files = []
tavily_client = TavilyClient(api_key=st.secrets["TAVILY_API_KEY"]) # ğŸ‘ˆ è¿˜æœ‰è¿™é‡Œï¼Œæ˜¯ä¸æ˜¯æ‹¼å†™ä¸å®Œæ•´ï¼Ÿ

# ==========================================
# ğŸš¨ ç»ˆæè¿›åŒ–ï¼šç½‘é¡µä¾§è¾¹æ ä¸Šä¼ ç»„ä»¶
# ==========================================
with st.sidebar:
    st.header("ğŸ“‚ è€ç‹çš„è®°å¿†æ’æ§½")
    uploaded_file = st.file_uploader("è¯·å–‚ç»™è€ç‹ä¸€ä»½æ–°çš„ PDF ç§˜ç±", type=["pdf"])

# åŠ¨æ€è¯»å–å¹¶ç¼“å­˜ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆæŠŠæ–‡ä»¶å­—èŠ‚æµä¼ è¿›æ¥ï¼Œåªè¦ä¼ äº†æ–°æ–‡ä»¶ï¼Œå°±ä¼šè‡ªåŠ¨åˆ·æ–°è„‘å­ï¼‰
@st.cache_resource(show_spinner=False)
def process_new_pdf(file_bytes, file_name):
    with open("temp_upload.pdf", "wb") as f:
        f.write(file_bytes)
        
    loader = PyPDFLoader("temp_upload.pdf") # è®°å¾—ä¸è¦åŠ  extract_images
    docs = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40)
    splits = text_splitter.split_documents(docs)
    
    embeddings = DashScopeEmbeddings(
        dashscope_api_key=st.secrets["API_KEY"], 
        model="text-embedding-v3" 
    )
    return FAISS.from_documents(splits, embeddings)

# --- ä¾§è¾¹æ ä¸Šä¼ é€»è¾‘æ›´æ–° ---
with st.sidebar:
    st.header("ğŸ“‚ è€ç‹çš„æ°¸ä¹…è®°å¿†åº“")
    # åŠ ä¸Šäº†å¤šé€‰æ¡†åŠŸèƒ½ï¼Œè§†è§‰æ›´çˆ½
    uploaded_file = st.file_uploader("ä¸Šä¼ ã€Šè½¯ä»¶è®¾è®¡å¸ˆã€‹æ–°èµ„æ–™", type=["pdf"])
    
    # å¿…é¡»ç‚¹å‡»æŒ‰é’®æ‰å¼€å§‹å­¦ä¹ ï¼Œé˜²æ­¢å¡é¡¿
    if st.button("ğŸ§  å¼€å§‹èåˆå­¦ä¹ ") and uploaded_file is not None:
        if uploaded_file.name in st.session_state.learned_files:
            st.warning(f"è¿™ä»½ã€Š{uploaded_file.name}ã€‹è€ç‹å·²ç»å€’èƒŒå¦‚æµå•¦ï¼")
        else:
            with st.spinner(f"æ­£åœ¨å°†ã€Š{uploaded_file.name}ã€‹èå…¥å¤§è„‘..."):
                try:
                    # å¬å”¤æ–°çš„å‡½æ•°
                    new_db = process_new_pdf(uploaded_file.getvalue(), uploaded_file.name)
                    
                    # è®°å¿†ç¼åˆé€»è¾‘
                    if st.session_state.vectorstore is None:
                        st.session_state.vectorstore = new_db
                    else:
                        st.session_state.vectorstore.merge_from(new_db)
                    
                    st.session_state.learned_files.append(uploaded_file.name)
                    st.success(f"âœ… æˆåŠŸèåˆï¼ç›®å‰å·²æŒæ¡ {len(st.session_state.learned_files)} ä»½èµ„æ–™ã€‚")
                except Exception as e:
                    st.error(f"âŒ æŠ“åˆ°çœŸå‡¶äº†ï¼çœŸå®æŠ¥é”™æ˜¯ï¼š{str(e)}")

    # å±•ç¤ºå·²ç»å­¦è¿‡çš„ä¹¦å•
    if st.session_state.learned_files:
        st.write("---")
        st.write("ğŸ“š ç›®å‰å·²æŒæ¡çš„çŸ¥è¯†ï¼š")
        for f_name in st.session_state.learned_files:
            st.caption(f"â€¢ {f_name}")

# ä¿®æ”¹å‰é¢çš„ vectorstore åˆ¤æ–­é€»è¾‘
vectorstore = None
if uploaded_file is not None:
    with st.spinner("è€ç‹æ­£åœ¨ç–¯ç‹‚é€Ÿè¯» PDF..."):
        try:
            vectorstore = load_knowledge_base(uploaded_file.getvalue())
            st.sidebar.success("âœ… ç§˜ç±å¸æ”¶å®Œæ¯•ï¼å¯éšæ—¶æé—®ã€‚")
        except Exception as e:
            # ğŸš¨ æ ¸å¿ƒä¿®æ”¹ï¼šè®©å®ƒæŠŠçœŸå®çš„æŠ¥é”™ä»£ç ååœ¨å±å¹•ä¸Šï¼
            st.sidebar.error(f"âŒ æŠ“åˆ°çœŸå‡¶äº†ï¼çœŸå®æŠ¥é”™æ˜¯ï¼š{str(e)}")
            st.sidebar.error(f"æŠ¥é”™ç±»å‹ï¼š{type(e)}")
else:
    st.sidebar.info("ğŸ‘ˆ è¯·å…ˆä¸Šä¼  PDFï¼Œå¦åˆ™è€ç‹çš„ç§æœ‰è®°å¿†åº“æ˜¯ç©ºçš„å“¦ï¼")

# ==========================================
# ğŸ› ï¸ æŠ€èƒ½ 1ï¼šå…¬ç½‘æœç´¢ (ä¿æŒä¸å˜)
# ==========================================
@tool
def web_search(query: str) -> str:
    """å½“éœ€è¦æŸ¥è¯¢å®æ—¶ä¿¡æ¯ã€æ–°é—»ã€ä¸çŸ¥é“çš„å®¢è§‚çŸ¥è¯†æ—¶ï¼Œè°ƒç”¨æ­¤å·¥å…·å…¨ç½‘æœç´¢ã€‚"""
    try:
        response = tavily_client.search(query=query, search_depth="basic", max_results=2)
        return "\n\n".join([f"æ ‡é¢˜: {res['title']}\nå†…å®¹: {res['content']}" for res in response['results']])
    except Exception as e:
        return f"æœç´¢å¤±è´¥ï¼š{str(e)}"

# ==========================================
# ğŸ› ï¸ æŠ€èƒ½ 2ï¼šç§æœ‰çŸ¥è¯†åº“æœç´¢ (å¢åŠ åˆ¤ç©ºé€»è¾‘)
# ==========================================
@tool
def search_internal_doc(query: str) -> str:
    """å½“ç”¨æˆ·è¯¢é—®å…³äºä¸Šä¼ çš„PDFæ–‡ä»¶ã€å†…éƒ¨çŸ¥è¯†ã€å¤ä¹ èµ„æ–™æ—¶ï¼Œè°ƒç”¨æ­¤å·¥å…·ã€‚"""
    # ğŸš¨ è¿™é‡Œæ”¹ç”¨ session_state é‡Œçš„å…¨å±€è„‘å­
    if st.session_state.vectorstore is None:
        return "è¯·ç¤¼è²Œåœ°å‘Šè¯‰ç”¨æˆ·ï¼šè€ç‹ç›®å‰è„‘å­é‡Œç©ºç©ºå¦‚ä¹Ÿï¼Œè¯·å…ˆä¸Šä¼  PDF èµ„æ–™ï¼"
    
    retriever = st.session_state.vectorstore.as_retriever()
    results = retriever.invoke(query)
    return "\n\n".join([res.page_content for res in results])

# å°†ä¸¤ä¸ªæŠ€èƒ½è£…è¿›å¤§è„‘
agent_executor = create_react_agent(llm, [web_search, search_internal_doc])


# ---------------- ä¸‹é¢æ˜¯ç½‘é¡µç•Œé¢çš„å¸¸è§„é€»è¾‘ ----------------
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¹½é»˜çš„å…¨èƒ½AIåŠ©ç†è€ç‹ã€‚"}]

for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

if user_input := st.chat_input("è¯•è¯•è¿æ‹›ï¼šä»Šå¤©çš„å¾®åšçƒ­æœæ˜¯ä»€ä¹ˆï¼Ÿé‚£è½¯ä»¶è®¾è®¡å¸ˆçš„å£è¯€å‘¢ï¼Ÿ"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("è€ç‹æ­£åœ¨å·¦å³è„‘åŒæ—¶è¿è½¬..."):
            response = agent_executor.invoke({"messages": st.session_state.messages})
            ai_reply = response["messages"][-1].content
            st.markdown(ai_reply)
            
    st.session_state.messages.append({"role": "assistant", "content": ai_reply})